\chapter{Opis projektu i jego rozwi¹zania}
\section{Temat i analiza zadania projektowego}

Zadaniem projektowym jest stworzenie systemu do rozpoznawania cyfr pisanych rêcznie z u¿yciem sieci MLP. Dane bêd¹ dane jako zbiór MNIST.


Dane zadanie jest problemem odpowiedniego przyporz¹dkowania obrazków do odpowiadaj¹ych im kategorii na podstawie cyfr tworzonych przez ich piksele. 
Nale¿y wiêc stworzyæ klasyfikator, który poprawnie przydzieli dany obrazek do jednej z 10 klas (ka¿da cyfra z przedzia³u $0-9$ tworzy jedn¹ klasê).
Zadanie zostanie rozwi¹zane poprzez stworzenie sieci neuronowej przy pomocy biblioteki TensorFlow.

\section{Techniczne szczegó³y zastosowanem sieci}
\subsection{Ogólna struktura sieci}
Zadanie to rozwi¹¿emy stosuj¹c sieæ neurowno¹ MLP (Multi Layer Perceptron), to jest sieæ neuronow¹ typu \textit{feedforward}, zawieraj¹c¹ jedn¹ lub wiêcej wartw ukrytych sztucznych neuronów. Jej struktura pokazana jest na rys. \ref{netstructure}.
\newline
\begin{center}
	\includegraphics[width=15cm]{utils/tikz11.png}
	\captionof{figure}{Struktura sieci MLP}
	\label{netstructure}
\end{center}

\subsection{Szcztuczny neuron}
Pojedyñczy neuron pokazany jest na rys. \ref{neuron}. Wykonuje on iloczyn skalarny wektorów tworzonych przez wartoœci wejœæ $x_j$ i odpowiadaj¹ce im wagi $w_j$ po czym dodaje do niego wielkoœæ \textit{bias}:

\begin{equation}
\sum w_jx_j + bias
\end{equation}

Tak otrzymana wartoœæ jest podawana jako argument funkcji wzbudzenia neuronu. Dopiero wynik tej funkcji jest wynikiem koñcowytm neuronu, \textit{output} na rys. \ref{neuron}.
\newline
\begin{center}
	\includegraphics[width=9cm]{utils/neuron.png}
	\captionof{figure}{Pojedyñczy neuron}
	\label{neuron}
\end{center}

\subsection{Funkcja wzbudzenia neuronów}
Zastosowana funkcja wzbudzenia naszej sieci MLP bêdzie mia³a postaæ funkcji sigmoidalnej, przedstawionej na rys. \ref{sigm}.
Funkcja tej postaci zapewnia nam dwie korzyœci:

\begin{enumerate}
	
	\item Ma³e zmiany wejœæ neuronów bêd¹ skutkowa³y niewielkimi zmianami na wyjœciu funkcji sigmoidalnej; pozwoli to kontrolowaæ proces uczenia (w przypadku np. funkcji pobudzenia postaci funkcji skokowej, nawet ma³e zmiany wejœæ mog¹ powodowaæ nieprzewidywalne zachowanie ca³ej sieci);
	\item Funkcja ta jest ró¿niczkowalna, co jest kluczowe w algorytmie uczenia wykorzystuj¹cym mechanizm wstecznej propagacji (ang. \textit{backpropagation}).
	
\end{enumerate}
\vspace{1cm}
\begin{center}
	\includegraphics[width=13cm]{utils/sigm.png}
	\captionof{figure}{Postaæ funkcji sigmoidalnej}
	\label{sigm}
\end{center}


\subsection{Warstwa wejœciowa}
Zbiór MNIST sk³ada siê z 70,000 obrazków przedstawiaj¹cych pisane rêcznie cyfry. Ka¿dy z obrazków ma wymiary 28x28, co daje  iloœæ  $28*28 = 784$ pikseli na ka¿dej ilustracji. Nasza sieæ bêdzie posiada³a dok³adnie tyle wejœæ - ka¿dy z neuronów warstwy wejœciowej (\textit{input layer} na rys. \ref{netstructure}) bêdzie otrzymywa³ dane z jednego i tego samego dla wszystkich danych pikselu obrazka.
\subsection{Warstwa wyjœciowa}
Warstwa wyjœciowa natomiast, (\textit{output layer} na rys. \ref{netstructure}), bêdzie sk³ada³a siê z 10 neuronów: ka¿dy z nich bêdzie sygnalizowa³ stopieñ przynale¿noœci obrazka do danej klasy.

Stopieñ przynale¿noœci, jako konsekwencja zastosowania sigmoidalnej funkcji wzbudzeñ neuronów, bêdzie zawiera³ siê w przedziale $<0, 1>$. Najwiêksza wartoœci ze zbioru neuronów wyjœciowych bêdzie identyfikowa³a aktualnie identyfikowan¹ cyfrê.

\subsection{Proces uczenia}
Do nauki sieci zostanie wykorzystany algorytm wstecznej propagacji (ang. \textit{backpropagation}). Zadanie uczenie sieci przy pomocy tego algorytmu polaga na minimalizacji funckji straty (ang. \textit{loss funtion}) jako funkcji wag i parametrów bias. Funkcja ta zosta³a przedstawiona jako wzór  \ref{lossf}, gdzie: $w$ - wagi, $b$ - bias, $n$ - liczebnoœæ zbioru ucz¹cego, $y(x)$ - po¿¹dane wyjœcie dla danego wejœcia,  $a$ - rzeczywisty wynik dzia³ania sieci dla danego wejœcia.

Algorytm ten polega na aplikowaniu do wag drobnych poprawek $\Delta w$ wyliczonych ze wzoru \ref{deltav}, gdzie $ \nabla C$ jest gradientem funkcji strat a  $\eta$ jest hiperparametrem wyra¿aj¹cym wielkoœæ kroku. 

Skrótowo mówi¹c, po ka¿dej iteracji, to jest obliczeniu wyniku dla kolejno wszystkich danych ucz¹cych, obliczana jest funkcja $C(w,b)$ i jej gradient, a nastêpnie zmieniane s¹ wagi w kierunku przeciwnym do  gradientu funcji strat o krok $\eta$.

\begin{eqnarray}  C(w,b) \equiv
\frac{1}{2n} \sum_x \| y(x) - a\|^2.
\label{lossf}
\end{eqnarray}

\begin{eqnarray}
\Delta w = -\eta \nabla C,
\label{deltav}
\end{eqnarray}


